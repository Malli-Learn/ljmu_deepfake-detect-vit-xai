{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":7332880,"sourceType":"datasetVersion","datasetId":4256754}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Install required Packages","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip install torchbnn\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install vit-pytorch","metadata":{"execution":{"execution_failed":"2025-05-14T06:54:17.422Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install vit-pytorch\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install captum\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.  Setup","metadata":{}},{"cell_type":"code","source":"# Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom captum.attr import IntegratedGradients, visualization\nfrom vit_pytorch import ViT\nimport platform\n\n# System Info  for exp setup \nprint(\"System:\", platform.platform())\nprint(\"Python version:\", platform.python_version())\nprint(\"Torch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Load DFDC Data","metadata":{}},{"cell_type":"code","source":"# Dataset Class\nclass ASDataset(Dataset):\n    def __init__(self, client_file: str, imposter_file: str, transforms=None):\n        with open(client_file, \"r\") as f:\n            client_files = f.read().splitlines()\n        with open(imposter_file, \"r\") as f:\n            imposter_files = f.read().splitlines()\n\n        self.labels = torch.cat((torch.ones(len(client_files)), torch.zeros(len(imposter_files))))\n        self.imgs = client_files + imposter_files\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, idx):\n        img_name = self.imgs[idx]\n        img = Image.open(img_name).convert(\"RGB\")\n        label = self.labels[idx]\n        if self.transforms:\n            img = self.transforms(img)\n        return img, label\n\n# Image Preprocessing\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\n# Load Data\ntrain_dataset = ASDataset(\n    client_file=\"/kaggle/input/dfdcdfdc/TRAIN_CLIENT.txt\",\n    imposter_file=\"/kaggle/input/dfdcdfdc/TRAIN_IMPOSTER.txt\",\n    transforms=preprocess\n)\n\nval_dataset = ASDataset(\n    client_file=\"/kaggle/input/dfdcdfdc/TEST_CLIENT.txt\",\n    imposter_file=\"/kaggle/input/dfdcdfdc/TEST_IMPOSTER.txt\",\n    transforms=preprocess\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Training  ","metadata":{}},{"cell_type":"code","source":"# Training Function\n\ndef train_model(model, optimizer, criterion, loader, num_epochs=5):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for inputs, labels in tqdm(loader, desc=f\"Epoch {epoch+1}\"):\n            optimizer.zero_grad()\n            outputs = model(inputs.to(device))\n            loss = criterion(outputs, labels.unsqueeze(-1).to(device))\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        print(f\"Epoch {epoch+1}: Loss = {running_loss / len(loader):.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluation Function\n\ndef evaluate_model(model, loader):\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for inputs, labels in loader:\n            outputs = model(inputs.to(device))\n            preds = torch.sigmoid(outputs).cpu().numpy()\n            all_preds.extend(preds.flatten())\n            all_labels.extend(labels.numpy())\n    auc = roc_auc_score(all_labels, all_preds)\n    accuracy = accuracy_score(all_labels, (np.array(all_preds) > 0.5).astype(int))\n    return {\"AUC\": auc, \"Accuracy\": accuracy}","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.ViT Experiments with differnet Configs and Model Saving","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncriterion = nn.BCEWithLogitsLoss()\nvit_params = [\n    {\"dim\": 512, \"depth\": 6, \"heads\": 8},\n    {\"dim\": 768, \"depth\": 8, \"heads\": 12},\n    {\"dim\": 1024, \"depth\": 12, \"heads\": 16}\n]\n\nvit_results = []\n\nfor i, params in enumerate(vit_params):\n    print(f\"\\n--- Training ViT Config {i+1}: {params} ---\")\n\n    vit_model = ViT(\n        image_size=224,\n        patch_size=32,\n        num_classes=1,\n        dim=params[\"dim\"],\n        depth=params[\"depth\"],\n        heads=params[\"heads\"],\n        mlp_dim=2048,\n        dropout=0.1,\n        emb_dropout=0.1,\n    ).to(device)\n\n    vit_optimizer = optim.Adam(vit_model.parameters(), lr=1e-4, weight_decay=5e-4)\n    \n    train_model(vit_model, vit_optimizer, criterion, train_loader, num_epochs=5)\n\n    # Save the trained model\n    model_filename = f\"/kaggle/working/vit_config_{i+1}.pt\"\n    torch.save(vit_model.state_dict(), model_filename)\n    print(f\" Saved model to: {model_filename}\")\n\n    metrics = evaluate_model(vit_model, val_loader)\n    \n    vit_results.append({\n        \"config\": params,\n        \"metrics\": metrics,\n        \"model_path\": model_filename\n    })\n\n# Optional: Display summary of results\nfor result in vit_results:\n    print(f\"Config: {result['config']} → AUC: {result['metrics']['AUC']:.4f}, Accuracy: {result['metrics']['Accuracy']:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. XAI, Inferences ","metadata":{}},{"cell_type":"code","source":"from vit_pytorch import ViT\n\nconfig = {\"dim\": 768, \"depth\": 8, \"heads\": 12}\nmodel_path = \"/kaggle/working/vit_config_2.pt\"\n\nvit_model = ViT(\n    image_size=224,\n    patch_size=32,\n    num_classes=1,\n    dim=config[\"dim\"],\n    depth=config[\"depth\"],\n    heads=config[\"heads\"],\n    mlp_dim=2048,\n    dropout=0.1,\n    emb_dropout=0.1,\n).to(device)\n\nvit_model.load_state_dict(torch.load(model_path))\nvit_model.eval()\nprint(\"Model loaded for XAI or inference\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6.  Metrics Summary","metadata":{}},{"cell_type":"code","source":"print(\"\\n=== ViT Configuration Results ===\")\nfor i, result in enumerate(vit_results):\n    config = result['config']\n    metrics = result['metrics']\n    print(f\"Config {i+1}: dim={config['dim']}, depth={config['depth']}, heads={config['heads']} → \"\n          f\"AUC: {metrics['AUC']:.4f}, Accuracy: {metrics['Accuracy']:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Metric comparision ","metadata":{}},{"cell_type":"code","source":"# Plot Metrics Comparison \nlabels = [f\"d={r['config']['dim']},L={r['config']['depth']},H={r['config']['heads']}\" for r in vit_results]\nauc_scores = [r['metrics']['AUC'] for r in vit_results]\nacc_scores = [r['metrics']['Accuracy'] for r in vit_results]\n\nx = range(len(labels))\nwidth = 0.35\nplt.figure(figsize=(10, 6))\nplt.bar(x, auc_scores, width=width, label='AUC', alpha=0.7)\nplt.bar([p + width for p in x], acc_scores, width=width, label='Accuracy', alpha=0.7)\nplt.xticks([p + width/2 for p in x], labels, rotation=30)\nplt.ylabel(\"Score\")\nplt.title(\"ViT Configurations - AUC and Accuracy\")\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. XAI with Integrated Gradients (IG)","metadata":{}},{"cell_type":"code","source":"from captum.attr import IntegratedGradients\n\n# Load a model (e.g., best one - config 2)\nmodel_path = vit_results[1]['model_path']  # Config 2\nconfig = vit_results[1]['config']\n\nmodel = ViT(\n    image_size=224,\n    patch_size=32,\n    num_classes=1,\n    dim=config[\"dim\"],\n    depth=config[\"depth\"],\n    heads=config[\"heads\"],\n    mlp_dim=2048,\n    dropout=0.1,\n    emb_dropout=0.1,\n).to(device)\n\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()\n\n# Use a sample image\nsample_input, _ = next(iter(val_loader))\nsample_image = sample_input[0].unsqueeze(0).to(device)\n\n# Generate attribution map\nig = IntegratedGradients(model)\nattributions = ig.attribute(sample_image, target=0)\n\n# Unnormalize and visualize\noriginal = sample_image.squeeze().cpu().detach().numpy()\noriginal = np.transpose(original, (1, 2, 0))\noriginal = (original * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\n\nattr_map = attributions.squeeze().cpu().detach().numpy()\nattr_map = np.transpose(attr_map, (1, 2, 0))\n\n# Plot both images\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\naxs[0].imshow(original.clip(0, 1))\naxs[0].set_title(\"Original Image\")\naxs[0].axis(\"off\")\n\naxs[1].imshow(original.clip(0, 1))\naxs[1].imshow(np.mean(attr_map, axis=-1), cmap='hot', alpha=0.6)\naxs[1].set_title(\"XAI Heatmap Overlay (IG)\")\naxs[1].axis(\"off\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. XAI with LIME (Random Image)","metadata":{}},{"cell_type":"code","source":"import random\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\n\n# Pick a random image from the dataset\nrandom_idx = random.randint(0, len(val_dataset) - 1)\nimg_tensor, _ = val_dataset[random_idx]\n\n# Convert tensor to numpy image\nimage_np = np.transpose(img_tensor.numpy(), (1, 2, 0))\nimage_np = (image_np * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\nimage_np = np.uint8(np.clip(image_np * 255, 0, 255))\n\n# Define LIME prediction wrapper\ndef predict_fn(images):\n    model.eval()\n    images_tensor = torch.stack([preprocess(Image.fromarray(img)) for img in images]).to(device)\n    with torch.no_grad():\n        outputs = model(images_tensor)\n        probs = torch.sigmoid(outputs).cpu().numpy()\n    return np.concatenate([1 - probs, probs], axis=1)\n\n# Run LIME\nexplainer = lime_image.LimeImageExplainer()\nexplanation = explainer.explain_instance(image_np, predict_fn, top_labels=1, hide_color=0, num_samples=1000)\n\n# Show original and LIME output\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].imshow(image_np)\naxes[0].set_title(\"Original Image\")\naxes[0].axis('off')\n\ntemp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, hide_rest=False)\naxes[1].imshow(mark_boundaries(temp, mask))\naxes[1].set_title(\"LIME Explanation\")\naxes[1].axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from captum.attr import IntegratedGradients, visualization\n# import matplotlib.pyplot as plt\n# import numpy as np\n# import torch\n\n# # Ensure the model is in evaluation mode\n# model = vit_results[0]['model']\n# model.eval()\n\n# # Get a sample image from validation loader\n# sample_input, _ = next(iter(val_loader))   # batch of images\n# sample_image = sample_input[0].unsqueeze(0).to(device)  # pick the first image and move to device\n\n# # Set up Integrated Gradients\n# ig = IntegratedGradients(model)\n\n# # Generate attributions\n# attributions = ig.attribute(sample_image, target=0)\n\n# # Convert tensors to numpy arrays\n# original = sample_image.squeeze().cpu().detach().numpy()\n# original = np.transpose(original, (1, 2, 0))  # CHW to HWC\n\n# attr_map = attributions.squeeze().cpu().detach().numpy()\n# attr_map = np.transpose(attr_map, (1, 2, 0))  # CHW to HWC\n\n# # Plot side-by-side\n# fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n# axs[0].imshow((original * 0.229 + 0.485).clip(0, 1))  # un-normalize\n# axs[0].set_title(\"Original Input Image\")\n# axs[0].axis(\"off\")\n\n# axs[1].imshow((original * 0.229 + 0.485).clip(0, 1))\n# axs[1].imshow(np.mean(attr_map, axis=-1), cmap='hot', alpha=0.6)\n# axs[1].set_title(\"XAI Heatmap Overlay (IG)\")\n# axs[1].axis(\"off\")\n\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LIME for ViT","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import random\n# from lime import lime_image\n# from skimage.segmentation import mark_boundaries\n# import matplotlib.pyplot as plt\n\n# # Select a random index from the full validation dataset\n# random_idx = random.randint(0, len(val_dataset) - 1)\n# img_tensor, _ = val_dataset[random_idx]\n\n# # Convert tensor to numpy image for LIME\n# image_np = np.transpose(img_tensor.numpy(), (1, 2, 0))\n# image_np = (image_np * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n# image_np = np.uint8(np.clip(image_np * 255, 0, 255))\n\n# # Define prediction wrapper for LIME\n# def predict_fn(images):\n#     model.eval()\n#     images_tensor = torch.stack([preprocess(Image.fromarray(img)) for img in images]).to(device)\n#     with torch.no_grad():\n#         outputs = model(images_tensor)\n#         probs = torch.sigmoid(outputs).cpu().numpy()\n#     return np.concatenate([1 - probs, probs], axis=1)\n\n# # Run LIME\n# explainer = lime_image.LimeImageExplainer()\n# explanation = explainer.explain_instance(image_np, predict_fn, top_labels=1, hide_color=0, num_samples=1000)\n\n# # Visualize both original and LIME heatmap\n# fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n# # Original image\n# axes[0].imshow(image_np)\n# axes[0].set_title(\"Original Image\")\n# axes[0].axis('off')\n\n# # LIME explanation\n# temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, hide_rest=False)\n# axes[1].imshow(mark_boundaries(temp, mask))\n# axes[1].set_title(\"LIME Explanation\")\n# axes[1].axis('off')\n\n# plt.tight_layout()\n# plt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## 11. Define Models CNN , BNN and ViT","metadata":{}},{"cell_type":"markdown","source":"### A. CNN Model","metadata":{}},{"cell_type":"code","source":"## CNN Baseline Definition\n\nclass CNNBaseline(nn.Module):\n    def __init__(self):\n        super(CNNBaseline, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(64 * 56 * 56, 512)\n        self.fc2 = nn.Linear(512, 1)\n\n    def forward(self, x):\n        x = self.pool(nn.ReLU()(self.conv1(x)))\n        x = self.pool(nn.ReLU()(self.conv2(x)))\n        x = x.view(x.size(0), -1)\n        x = self.dropout(nn.ReLU()(self.fc1(x)))\n        return self.fc2(x)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### B. CNN Metics","metadata":{}},{"cell_type":"code","source":"cnn_model = CNNBaseline().to(device)\ncnn_optimizer = optim.Adam(cnn_model.parameters(), lr=1e-4, weight_decay=5e-4)\n\ntrain_model(cnn_model, cnn_optimizer, criterion, train_loader, num_epochs=5)\ncnn_metrics = evaluate_model(cnn_model, val_loader)\n\nprint(f\"\\nCNN Baseline → AUC: {cnn_metrics['AUC']:.4f}, Accuracy: {cnn_metrics['Accuracy']:.4f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### B. BNN Model","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# import torchbnn as bnn  # Ensure torchbnn is installed\n\n# class BNNModel(nn.Module):\n#     def __init__(self, prior_mu=0.0, prior_sigma=1.0):\n#         super(BNNModel, self).__init__()\n        \n#         # Bayesian convolutional layers with priors\n#         self.conv1 = bnn.BayesConv2d(\n#             in_channels=3, \n#             out_channels=32, \n#             kernel_size=3, \n#             stride=1, \n#             padding=1, \n#             prior_mu=prior_mu, \n#             prior_sigma=prior_sigma\n#         )\n#         self.conv2 = bnn.BayesConv2d(\n#             in_channels=32, \n#             out_channels=64, \n#             kernel_size=3, \n#             stride=1, \n#             padding=1, \n#             prior_mu=prior_mu, \n#             prior_sigma=prior_sigma\n#         )\n        \n#         # Pooling layer\n#         self.pool = nn.MaxPool2d(2, 2)\n        \n#         # Fully connected Bayesian layers with priors\n#         self.fc1 = bnn.BayesLinear(\n#             in_features=64 * 56 * 56, \n#             out_features=512, \n#             prior_mu=prior_mu, \n#             prior_sigma=prior_sigma\n#         )\n#         self.fc2 = bnn.BayesLinear(\n#             in_features=512, \n#             out_features=1, \n#             prior_mu=prior_mu, \n#             prior_sigma=prior_sigma\n#         )\n\n#     def forward(self, x):\n#         # Forward pass through Bayesian layers\n#         x = self.pool(nn.ReLU()(self.conv1(x)))  # Conv1 -> ReLU -> Pooling\n#         x = self.pool(nn.ReLU()(self.conv2(x)))  # Conv2 -> ReLU -> Pooling\n        \n#         # Flatten the output for fully connected layers\n#         x = x.view(x.size(0), -1)\n        \n#         # Fully connected layers\n#         x = nn.ReLU()(self.fc1(x))\n#         x = self.fc2(x)\n        \n#         return x","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 11. Metric Comparision","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extract ViT Config 2 metrics\nvit2_metrics = vit_results[1][\"metrics\"]\n\n# Prepare data\nmodels = ['CNN', 'ViT Config 2\\n(dim=768, depth=8, heads=12)']\nauc_scores = [cnn_metrics['AUC'], vit2_metrics['AUC']]\nacc_scores = [cnn_metrics['Accuracy'], vit2_metrics['Accuracy']]\n\n# Plot\nx = range(len(models))\nwidth = 0.35\n\nplt.figure(figsize=(8, 5))\nplt.bar(x, auc_scores, width=width, label='AUC', alpha=0.8)\nplt.bar([p + width for p in x], acc_scores, width=width, label='Accuracy', alpha=0.6)\n\nplt.xticks([p + width / 2 for p in x], models)\nplt.ylabel(\"Score\")\nplt.ylim(0, 1.1)\nplt.title(\"Model Performance Comparison: CNN vs ViT Config 2\")\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare AUC and Accuracy\nmetrics = {\n    \"Model\": [\"CNN\", \"ViT\"],\n    \"AUC\": [cnn_metrics[\"AUC\"], vit_metrics[\"AUC\"]],\n    \"Accuracy\": [cnn_metrics[\"Accuracy\"], vit_metrics[\"Accuracy\"]],\n}\n\n# Plot\nplt.bar(metrics[\"Model\"], metrics[\"AUC\"], alpha=0.6, label=\"AUC\")\nplt.bar(metrics[\"Model\"], metrics[\"Accuracy\"], alpha=0.6, label=\"Accuracy\")\nplt.xlabel(\"Model\")\nplt.ylabel(\"Metric\")\nplt.legend()\nplt.title(\"Model Comparison\")\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:54:17.423Z"}},"outputs":[],"execution_count":null}]}